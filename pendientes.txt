---------------------------------------------- PENDIENTES -----------------------------------
current objectives: max gu data rate
fecha: 11/09/2023
Pendientes DRONE2D single objective function 2 GUS DQN ..
revisar reward function
tengo muchas variables y objetos para almacenar la misma data (ej: gus, drone pos) modificare el codigo del robotarium para funcioanr
con mis metodologias de programacion. listo.
desarrollar el modulo para testear el algoritmo DQN entrenado. listo
grabar backups del archivo pickle que guarda los meanrewards por episodio. listo
optimizar el codigo ya que los entrenamientos estan costando demasiada memoria RAM. cada ciertos training steps los mean rewards por
episodio son guardados en un archivo. no encontre otra forma de reducir el costo computacional de tiempo



prob.e modelo v2 30 y 34, al inicio el drone es capaz de quedarse estacionario ya que sabe que los gus estan en su rango rc. sin embargo,
cuando uno de los gus se aleja, en ocasiones sigue al gu con mayor data rate, y en otras ocasiones se queda con el de menor data rate.
pero lo que siempre pasa despues de varias iteraciones, es que el drone decide alejarse bastante de los gus (fuera de rango rc).
falta mejorar entrenamiento.

ideas finalizar episodio de training:
1.- si algun gu o drone sobrepasan el area de entrenamiento (current)
2.- si drone se sobrepasa, penalizar reward func y no ejecutar dicha accion. si gu se pasa, end training
3.- cada episodio, gus aleatorios, y el drone debe ser capaz de cubrir a los gus.
4.- por n cantidad de timeslots

ideas movimientos gus:
1.- en el rango rc del drone, generamos aleatoriamente una posicion para cada gu. despues la accion de cada gu es desplazarse
una cantidad maxima en direccion aleatoria (current)
2.- tener un array con diferentes distribuciones de probabilidad (random, poisson, gaussian) y concatenar la accion con un array
de distancias maximas a ejecutar los gus. tomar decisiones random en estas 3 distribuciones

#---------------------------------------------
agregare una variable con el producto cartesiano entre las 3 distribuciones (calcular next dist gus) contra una direccion random.
listo, falta realizar mas tests
actualizar reward function


pruebas entrenamiento 13_09_2023 modelo 1 v2--9:
1.-las direcciones completamente random de los gus no estan del todo correcto, agregare unas condiciones donde una vez que el gu tomo una direccion, tendra ligeros cambios en ella, hasta
que una variable con cierta distribucion decida que es momento de cambiar a otra direction random.
2.-observe que los weights para la parte de data rate y distancia no deben de ser iguales. ya que el drone se inclina probablemente por el que tenga menos distancia, o puede ser que
debido a data rates de los gus muy proximos, no logra decidir seguir al correcto.

propuestas actualizacion algoritmo dqn y modelado del ambiente:
1.- cambiare la manera de finalizar el episodio de entrenamiento. ahora seran cierto numero de iteraciones, ademas los gus predeciran si su nueva posicion se encuentra fuera del rango, 
si es asi, entonces no tomara esa posicion en este timeslot. listo y verificado
1.1 probare cambiando la distribucion de los drones y gus. gus seran distribuidos aleatoriamente en el terreno ( o colocados en el rango rc) una probabilidad lo decidira. el drone,
conociendo posicion gus y su distancai relativa, se movera en el timeslot a cubrir alguno o ambos de los drones. listo.
2- reward function cambio, si el drone tomara una posicion fuera del target area, agregara una penalizacion, pero no tomara esa posicion.
listo
3.- probare con entrenamientos con menos episodiso y vere si el algoritmo esta aprendiendo. listo, falta probar
4.- probare con gama menor a 1 para ver si variar el peso de rewards futuros, evita que el drone sobre estime buenas decisiones, provocando que tome decisiones incorrectas precipitadamente. listo, falta probar
5.- aplicar observacion 1 en prueba 13_09_2023. listo
6.-reestructurar reward function. listo
7.- state space: incluir drone position, gus positions,listo

me falta guardar el tiempo de ejecucion por epoca. imprimirlo mientras entrena o guardarlo en un archivo. LISTO
pendientes:
optimizar sistema
realizar entrenamientos en google colab y evaluar tiempo por epoca. en paralelo, tratar de hacer funcionar alguna biblioteca de rl para el entrenamiento.

#------------------------- PENDIENTES REVISION COMPLETA DEL SISTEMA PARA OPTIMIZARLO ---------------------------------------------------------------- ######
funciones revision a detalle pendiente:
misc.poissonchoice deprecated, ahora desplazo una funcion gausiiana para simular una distribucion poisson
misc.gaussianchoice
environment.resetenv

parece ser que este conjunto de funciones requiere de mejorar sus tiempos.
parece ser que la funcion trainnetowrk es la que toma mucho tiempo. para 100 epocas toma ~.3 s, lo que incrementa exponencialmente el tiempo con el numero
de training slots y training eppochs. parece ser que trainnetwork() era mas rapido con CPU que con GPU.

train network execution time:
GPU windows 12/10/2023 : ~.45s
GPU linux 12/10/2023 : ~.65s
CPU colab 12/10/2023 : ~0.8s
GPU colab 12/10/2023 : ~0.78-1.5s

Episode running time:
CPU windows 12/10/2023 : ~32s batch 500 

funcino resetenv toma aprox 0.03 s. no es mucho. probablemente optimizando poisonchoice mejore el tiempo. tiempo mejoro a .001s

prueba: entrenar codigo en google colab (ya incluye gpu) y ver si el tiempo de entrenamiento reduce. prueba realizada: no mejoro. tiempo de exec por episodio: ~60s
#------------------------- PENDIENTES REVISION COMPLETA DEL SISTEMA PARA OPTIMIZARLO ---------------------------------------------------------------- ######
