---------------------------------------------- PENDIENTES -----------------------------------
ideas finalizar episodio de training:
1.- si algun gu o drone sobrepasan el area de entrenamiento (current)
2.- si drone se sobrepasa, penalizar reward func y no ejecutar dicha accion. si gu se pasa, end training
3.- cada episodio, gus aleatorios, y el drone debe ser capaz de cubrir a los gus.
4.- por n cantidad de timeslots

ideas movimientos gus:
1.- en el rango rc del drone, generamos aleatoriamente una posicion para cada gu. despues la accion de cada gu es desplazarse
una cantidad maxima en direccion aleatoria (current)
2.- tener un array con diferentes distribuciones de probabilidad (random, poisson, gaussian) y concatenar la accion con un array
de distancias maximas a ejecutar los gus. tomar decisiones random en estas 3 distribuciones

#---------------------------------------------
agregare una variable con el producto cartesiano entre las 3 distribuciones (calcular next dist gus) contra una direccion random.
listo, falta realizar mas tests
actualizar reward function


pruebas entrenamiento 13_09_2023 modelo 1 v2--9:
1.-las direcciones completamente random de los gus no estan del todo correcto, agregare unas condiciones donde una vez que el gu tomo una direccion, tendra ligeros cambios en ella, hasta
que una variable con cierta distribucion decida que es momento de cambiar a otra direction random.
2.-observe que los weights para la parte de data rate y distancia no deben de ser iguales. ya que el drone se inclina probablemente por el que tenga menos distancia, o puede ser que
debido a data rates de los gus muy proximos, no logra decidir seguir al correcto.

propuestas actualizacion algoritmo dqn y modelado del ambiente:
1.- cambiare la manera de finalizar el episodio de entrenamiento. ahora seran cierto numero de iteraciones, ademas los gus predeciran si su nueva posicion se encuentra fuera del rango, 
si es asi, entonces no tomara esa posicion en este timeslot. listo y verificado
1.1 probare cambiando la distribucion de los drones y gus. gus seran distribuidos aleatoriamente en el terreno ( o colocados en el rango rc) una probabilidad lo decidira. el drone,
conociendo posicion gus y su distancai relativa, se movera en el timeslot a cubrir alguno o ambos de los drones. listo.
2- reward function cambio, si el drone tomara una posicion fuera del target area, agregara una penalizacion, pero no tomara esa posicion.
listo
3.- probare con entrenamientos con menos episodiso y vere si el algoritmo esta aprendiendo. listo, falta probar
4.- probare con gama menor a 1 para ver si variar el peso de rewards futuros, evita que el drone sobre estime buenas decisiones, provocando que tome decisiones incorrectas precipitadamente. listo, falta probar
5.- aplicar observacion 1 en prueba 13_09_2023. listo
6.-reestructurar reward function. listo
7.- state space: incluir drone position, gus positions,listo


#---------------------------------------------------- PRUEBA FECHA : 10/10/2023 ENTRENAMIENTO SISTEMA USANDO TF-GENTS------------------------------------------------------------------------ #####

* durante los primeros episodios del testing, pudimos observar como el drone navega rumbo hacia los gus, sin embargo hubo casos donde los cuales, si los gus estan bastante separados, el drone se
inclina por el de mayor data rate (comportamiento deseado). de lo contrario, cuando estan muy cercanos, el drone visita uno y despues al otro. 
* Despues de ciertos numero de episodios, observamos que el agente es incapaz de escoger buenas posiciones, no se acerca a nigun gu.
* Cuando si se acercaba a los gus, notamos que toma ciertas decisiones que no son las optimas. Teoria: penalizar el numero de pasos que le toma al drone llegar a darle serivcio a algun gu.
* En ocasiones, da a aentender que el drone sabe que cuando esta con gu debe de ejecutar la accion de no moverse. Sin embargo, en otras ocaciones no. Teoria: agregando la penalizacion por numero de pasos
creemos que, una vez llegue con un gu, este sabra que debera quedarse estatico por el resto del episodio, para maximizar reward.
* ayudaremos un poco a vencer la incertidumbre entre penalizaciones y rewards aumentando el weight data rate.

#---------------------------------------------------- PRUEBA FECHA : 29/10/2023 ENTRENAMIENTO SISTEMA USANDO TF-GENTS------------------------------------------------------------------------ #####
MODELO 1 v3. rewardfunction3, weight_dr = 1.5, flying_ouside_penalization = 0.7, 

* La red neuronal claramente hace overfitting. Produciendo un comportamiento atipico, donde el agente solo toma tres decicions, avanzar hacia adelante, retroceder o permane
cer quieto. El origen de este comportamiento no me es claro.

Combinacion de mejoras que considero implementar:
* cambiar de algoritmo de entrenamiento (intentar con actor-critic)
* probar cambiando la estructura de la red neuronal Q (arquitectura, numero de capas, numero de neuronas, funciones de activacion en la capa ultima)
* regresar el escenario de entrenamiento al sistema donde, los gus aleatoriamente producen un cambio en su posicion, y el drone debera seguirlos
* reducir penalizacion por salirse del area
* agregar que el agente ejecute el menor numero de movimientos
* probar con otras rewards functinos

#---------------------------------------------------- PRUEBA FECHA : 29/10/2023 ENTRENAMIENTO SISTEMA USANDO TF-GENTS------------------------------------------------------------------------ #####
MODELO 1 v4. rewardfunction3, weight_dr = 1.0, flying_ouside_penalization = 0.5, 3 layer = (50,28,12)
* algo extraño sucede. entrenar el modelo hace que se estanque es 3 decisiones, avanzar retroceder o quedarse parado. 
* probare cambiando al entorno donde los gus se mueven,
* probare usando actor-critic


#---------------------------------------------------- PRUEBA FECHA : 29/10/2023 ENTRENAMIENTO SISTEMA USANDO TF-AGENTS------------------------------------------------------------------------ #####
MODELO 1 v3. rewardfunction3, weight_dr = 1.0, flying_ouside_penalization = 0.5, hidden_layer = (128,56,12)

Desde este entrenamiento corregi un detalle que tenia el main loop global. Cuando el sistema llegaba a un estado terminal , se guardaba
la trajectoria como final, pero el objeto py_driver no reseteaba el ambiente.
Ademas, note que el main loop de entrenamiento, no recolectaba episodios completos de entrenamiento, sino solamente steps.
corregi todo esto y el agente ya no toma decisiones solamente de avanzar, retroceder y parar.
* ra facilitar el entrenamiento, controlare mas el tamaño del cambio de distancia que pueden tener los gus, asi como que mantenan el data rate
por casi todo el episodio de training.

* El ssitema parece haber aprendido a quedarse parado cuando esten cercanos de el los gus, ademas de buscar al gu con mayor data rate. 
sin embargo, parece ser incapaz de quedarse con el por el resto del episodio.
* parece ser que el problema esque no converge el sistema.
